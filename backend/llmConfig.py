llm_config = {
    "model": "llama2",
    "api_key": "ollama", 
    "base_url": "http://localhost:11434/v1",
    "temperature": 0.3,
}